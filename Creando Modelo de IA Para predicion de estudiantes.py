# -*- coding: utf-8 -*-
"""Iniciando_Proyecto_Grado.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JllmmMwir69fv9CNyRkBf7rhKXX0aI8i

# Inicio
"""

import numpy as np
import pandas as pd
import seaborn as sns
import io
import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
## GridSearchCV busqueda en bruto de los hiperparametros
from sklearn.metrics import classification_report, f1_score, roc_curve, roc_auc_score, confusion_matrix, confusion_matrix,make_scorer,ConfusionMatrixDisplay

"""Arboles de decisión
Random Forest
Regresión logística
Redes neuronales
kvecinos mas cercanos
Naïve Bayes
Maquinas de soporte vectorial
"""

#Librerias de Algoritmos supervisados
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

#Librerias para equilibrar las clases
from imblearn.over_sampling import SMOTE

#Validacion de modelos 
#Estrategia de validacion cruzada para pocos datos >300
from sklearn.model_selection import LeaveOneOut


#metricas de evaluacion
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix



from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive
from google.colab import auth
from oauth2client.client import GoogleCredentials
# Authenticate and create the PyDrive client.
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

downloaded = drive.CreateFile({'id':'1QQ7F1EfMsA2ZrVYXqCnN1yUxU5r5Pkq5'}) 
downloaded.GetContentFile('Unificado_08-01-2022.csv')  
df = pd.read_csv('Unificado_08-01-2022.csv', sep=';', encoding="utf-8")
print(df.info())

"""# Limpieza

**Limpieza de los datos de Unificados**
"""

df.replace(0, np.nan, inplace=True)

df.CantidadMateriasPerdidas.replace(np.nan,0, inplace=True)
df.PorcentajeDeCreditos.replace(np.nan,0, inplace=True)
df.PROMEDIO.replace(np.nan,0, inplace=True)
df.ESTRATO.replace(np.nan,0, inplace=True)

df.PROMEDIO

df.ESTRATO= df.ESTRATO.astype(int)
df.CantidadMateriasPerdidas= df.CantidadMateriasPerdidas.astype(int)
df.PorcentajeDeCreditos= df.PorcentajeDeCreditos.astype(int)

# Eliminar Columnas cuyo valor de datos nulos superan el 95% y celdas que no se puedan recuperar 
df.drop(columns=['SOLI_ESTADO', 'SOLI_ID','PUNTAJE_SISBEN',
                 'DISCAPACIDAD_ID_PADRE','NOMBRE_EPS','MATRICULAS',
                 'MATERIA','NOTA','CREDITOS','CREDITOS_PROGRAMA','APROBADO',
                 'PERIODO'
                 ], inplace=True)

#Convertir en columna categorica en columna entera Pertenece a Un semillero
df.loc[df.SEMILLERO.notna(),'SEMILLERO']=1
df.loc[df.SEMILLERO.isnull(),'SEMILLERO']=0
df["SEMILLERO"]= df["SEMILLERO"].astype(int) 

#Convertir en columna categorica en columna entera Sexo Biologico
df.loc[df.SEXO=='M','SEXO']=1
df.loc[df.SEXO=='F','SEXO']=0
df["SEXO"]= df["SEXO"].astype(int) 

#Convertir columna categorica en columna entera CLASE_LIBRETA

print(df.groupby(['IDENTIFICADOR'])[['IDENTIFICADOR']].nunique())
# print(((df.isnull().sum() / len(df))*100).sort_values(ascending = False))

df.loc[df.SEXO == 0,'CLASE_LIBRETA']="N/A"
df.loc[(df.SEXO == 1) &(df.LIBRETA_MILITAR =='NO') & (df.CLASE_LIBRETA.isnull()),'CLASE_LIBRETA']="N/A"
df.loc[(df.SEXO == 1) &(df.LIBRETA_MILITAR =='SI') & (df.CLASE_LIBRETA.isnull()),'CLASE_LIBRETA']="No definido"

print(df.CLASE_LIBRETA.isnull().sum())

# print(df.loc[df.CLASE_LIBRETA=="Problemas"].IDENTIFICADOR.nunique())
# print(df.loc[df.LIBRETA_MILITAR=="SI"].IDENTIFICADOR.nunique())
print()
print(df.loc[df.SEXO==1].groupby(['CLASE_LIBRETA'])[['IDENTIFICADOR']].nunique())
((df.loc[df.SEXO==1].groupby(['CLASE_LIBRETA'])[['IDENTIFICADOR']].nunique()/df.loc[df.SEXO==1].groupby(['CLASE_LIBRETA'])[['IDENTIFICADOR']].nunique().sum())*100)

print(df.CLASE_LIBRETA.isnull().sum())

# print(df.CLASE_LIBRETA.isnull().count())

# Identificadores de personas con libreta militar sin especificar (con Problemas)
# Eliminar a los estudiantes hombres sin libreta militar
# df=df.loc[df.CLASE_LIBRETA!='Problemas']
# df.groupby(['FECHAGRADUACION'])[['IDENTIFICADOR']].nunique()

# df.FECHAGRADUACION.replace([np.nan],'N/A', inplace=True)

#df.drop(['IDENTIDAD_DE_GENERO','ORIENTACION_SEXUAL'], axis=1,inplace=True)
# print(((df.isnull().sum() / len(df))*100).sort_values(ascending = False))

#Eliminar columnas con los datos vacios que supera el 40% de los datos 
#df.drop(['DEPARTAMENTOID_INSTITUCION','MUNICIPIOID_INSTITUCION','NOMBRE_INSTITUCIONEDUCATIVA','INSTITUCIONEDUCATIVA'], axis=1,inplace=True)

print(((df.isnull().sum() / len(df))*100).sort_values(ascending = False))

df=df.reset_index()
df.index.max()

# Personas graduadas o excluidas que esten graduadas o que esten excluidas pero que tengan null "PAIS_NACIMIENTO","DEPARTAMENTO_NACIMIENTO","CIUDAD_NACIMIENTO","NACIONALIDAD"
df.loc[df.CIUDAD_NACIMIENTO.isnull()][["PERIODOINGRESO","IDENTIFICADOR","SITE_DESCRIPCION","PAIS_NACIMIENTO","DEPARTAMENTO_NACIMIENTO","CIUDAD_NACIMIENTO","NACIONALIDAD"]].loc[(df.SITE_DESCRIPCION=="GRADUADO") | (df.SITE_DESCRIPCION=="EXCLUIDO NO RENOVACION DE MATRICULA") ]
#Pendiente por llenar informacion

#Eliminar Registros nulos de ciudad de nacimiento debido a que hay
# 14 Personas sin PAIS_NACIMIENTO, DEPARTAMENTO_NACIMIENTO, CIUDAD_NACIMIENTO ,NACIONALIDAD
df=df.loc[df.CIUDAD_NACIMIENTO.notnull()]

#Ver ciudad de nacimiento de los usuarios sin Nacionalidad
print(df.loc[df.NACIONALIDAD.isnull()][["IDENTIFICADOR","PAIS_NACIMIENTO","DEPARTAMENTO_NACIMIENTO","CIUDAD_NACIMIENTO","NACIONALIDAD"]])

print('Nacionalidades existentes: '+
      df.loc[df.NACIONALIDAD.notnull()].NACIONALIDAD.unique())

#Cambiar archivos con datos nulos con pais de nacimiento de Colombia a nacionalidad colombiana 
df.loc[(df.NACIONALIDAD.isnull() & (df.PAIS_NACIMIENTO=="Colombia")),'NACIONALIDAD']='CO'

df.loc[(df.NACIONALIDAD=="AQ")|(df.NACIONALIDAD=="PE")|(df.NACIONALIDAD=="CO"),"NACIONALIDAD"]="Colombia"

df.loc[df.DISCAPACIDAD_DESCRI.isnull(),"DISCAPACIDAD_DESCRI"]="NO APLICA"

print(df.groupby(['DISCAPACIDAD_DESCRI'])[['IDENTIFICADOR']].nunique())

# CAPACIDADEXCEPCIONAL
df.loc[df.CAPACIDADEXCEPCIONAL.isnull(),"CAPACIDADEXCEPCIONAL"]="NO APLICA"
print(df.groupby(['CAPACIDADEXCEPCIONAL'])[['IDENTIFICADOR']].nunique())
print(df.IDENTIFICADOR.nunique())
#Se aplica el valor de que mas se repite

# print(df.groupby(['CREDITOS'])['MATERIA'].nunique().sort_values(ascending=True))

df.IDENTIFICADOR.unique()

# df.loc[df.CAPACIDADEXCEPCIONAL.isnull(),"CAPACIDADEXCEPCIONAL"]="NO APLICA"
print(df.groupby("DISCAPACIDAD")['IDENTIFICADOR'].nunique())

print(df.groupby("DISCAPACIDAD_DESCRI")['IDENTIFICADOR'].nunique())

#-df.drop(["DISCAPACIDAD","ZONARESIDENCIAL","PAIS_PROCEDENCIA","DEPARTAMENTO_PROCEDENCIA","MUNICIPIO_PROCEDENCIA"], axis=1,inplace=True)

# print(df.groupby((df.PUNTAJE_ICFES=="0")|(df.PUNTAJE_ICFES.isnull()))['IDENTIFICADOR'].nunique())

print(((df.isnull().sum() / len(df))*100).sort_values(ascending = False))

df.head()

df.groupby(['PROGRAMA'])[['IDENTIFICADOR']].nunique()

df.index.nunique()

Serie=((df.isnull().sum() / len(df))*100).sort_values(ascending = False)
Serie.loc[Serie.values>3]

# Eliminar Pais de procedencia
# Logica difusa
# Procentaje  


"""print(DD)
contador=0
ContadorDeFilas=0
indexFilas=[]
for x in DD:
  ContadorDeFilas=ContadorDeFilas + 1
  if x != False:
    indexFilas.append(ContadorDeFilas)
    contador=contador+1

print(contador)
print(indexFilas)
"""
# print(DD.columns)
##print(DD.loc[:,True])
##print(df.iloc[0])

#Eliminar columnas con datos blancos
series=((df.isnull().sum() / len(df))*100).sort_values(ascending = False)

series.loc[series>3].index

#
df=df.drop(series.loc[series>3].index,axis=1)

# Columnas Eliminadas
arreglonuevo=np.array(series.index)
seriese=np.array(df.columns)

for i, x in enumerate(seriese):
  for j, y in enumerate(arreglonuevo):
    if(x==y):
      arreglonuevo = np.delete(arreglonuevo, j)

arreglonuevo

((df.isnull().sum() / len(df))*100).sort_values(ascending = False)

df

((df.isnull().sum() / len(df))*100).sort_values(ascending = False)

#Buscar moda de la columna Seleccione_un_rango_de_costo_en_pasajes_hasta_la_universidad:
#Teniendo encuenta cada periodo academico y el transporte que se utilizo para desplazarse a la Universidad

# df.to_csv("Unificado_not_null.csv",sep=";", encoding='utf-8')

df.SITE_DESCRIPCION.unique()

dfFiltro=df.loc[(df.SITE_DESCRIPCION=="EXCLUIDO NO RENOVACION DE MATRICULA") | 
       (df.SITE_DESCRIPCION=="GRADUADO") | (df.SITE_DESCRIPCION=="EXCLUIDO CANCELACION SEMESTRE") |
       (df.SITE_DESCRIPCION=="INACTIVO")]

df.SITE_DESCRIPCION.unique()

df.groupby(['SITE_DESCRIPCION'])[['index']].nunique()

df.IDENTIFICADOR.shape

df.to_csv("Unificado.csv",sep=";", encoding='utf-8')

df.loc[df.SITE_DESCRIPCION =="GRADUADO"].IDENTIFICADOR.nunique()

df.columns.unique()

df.to_csv("DatosUnificadosYLimpios.csv",sep=";", encoding='utf-8')

"""# Entrenamiento del modelo teniendo encuenta la literatura

## Preprocesamiento del dataset
"""

dfFiltro=df.loc[(df.SITE_DESCRIPCION=="EXCLUIDO NO RENOVACION DE MATRICULA") | 
       (df.SITE_DESCRIPCION=="GRADUADO") | (df.SITE_DESCRIPCION=="EXCLUIDO CANCELACION SEMESTRE") |
       (df.SITE_DESCRIPCION=="INACTIVO")]

# Si trabaja, quien financia sus estudios, Si tiene conexion a internet y si tiene un pc permanente

dfFiltro=dfFiltro[['EDAD_DE_INGRESO','SEXO','Tipo_de_población_a_la_que_pertenece','ESTADO_CIVIL','Actualmente_trabaja','¿Cómo_financia_sus_estudios?','CIRCUNSCRIPCION','ESTRATO','¿Dispone_de_un_computador_permanentemente?',
                   '¿Posee_conexión_permanente_a_internet?','SITE_DESCRIPCION']]

dfFiltro.columns.nunique()

print(((dfFiltro.isnull().sum() / len(dfFiltro))*100).sort_values(ascending = False))

dfFiltro.Actualmente_trabaja.unique()

dfFiltro["EDAD_DE_INGRESO"]=dfFiltro["EDAD_DE_INGRESO"].astype(int) 
dfFiltro["EDAD_DE_INGRESO"]=pd.cut(dfFiltro["EDAD_DE_INGRESO"],[0,18,24,34,44,54],labels=[0,1,2,3,4])
dfFiltro["EDAD_DE_INGRESO"]=dfFiltro["EDAD_DE_INGRESO"].astype(int) 
#Columna de Sexo 

dfFiltro.Actualmente_trabaja.replace(['Sí', 'No'],[1,0], inplace=True)
dfFiltro.Tipo_de_población_a_la_que_pertenece.replace(["Urbana","Rural"],[1,0], inplace=True)

dfFiltro["ESTADO_CIVIL"].replace(['Soltero', 'Unión Libre', 'Madre soltera', 'Casado', 'Religioso','Divorciado'],[0,1,2,3,4,5], inplace=True)

dfFiltro['¿Cómo_financia_sus_estudios?'].replace(['Familia', 'Crédito', 'Recursos Propios', 'Beca'],[0,1,2,3], inplace=True)

dfFiltro['CIRCUNSCRIPCION'].replace(['REGULAR PREGRADO', 'DESPLAZADO', 'PUEBLO INDIGENA',
                                     'COMUNIDAD NEGRA', 'VICTIMA DEL CONFLICTO ARMADO INTERNO'],
                                    [0,1,2,3,4], inplace=True)

# Columna de Estrato 

dfFiltro['¿Dispone_de_un_computador_permanentemente?'].replace(['Sí', 'No'],[1,0], inplace=True)

dfFiltro['¿Posee_conexión_permanente_a_internet?'].replace(['Sí', 'No'],[1,0], inplace=True)

dfFiltro.loc[dfFiltro.SITE_DESCRIPCION!="GRADUADO","SITE_DESCRIPCION"]=0
dfFiltro.loc[dfFiltro.SITE_DESCRIPCION=="GRADUADO","SITE_DESCRIPCION"]=1
dfFiltro["SITE_DESCRIPCION"]=dfFiltro["SITE_DESCRIPCION"].astype(int)

# KFold Validation experimentar con distintos hiperparametros

dfFiltro

"""## Entrenamiento sin SMOTE

### Creación de los conjuntos de entrenamiento y prueba
"""

X=np.array(dfFiltro.drop(['SITE_DESCRIPCION'],1))
y=np.array(dfFiltro.SITE_DESCRIPCION)

def ParticionNormal():
  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)
  return X_train, X_test, y_train, y_test
X_train, X_test, y_train, y_test =ParticionNormal()

print(X_train.shape)
print(X_test.shape)

# Clase objetivo del dataset de entrenamiento y su distribucion
pd.Series(y_train).value_counts()

# Clase objetivo del dataset de prueba y su distribucion
pd.Series(y_test).value_counts()

# Distribucion grafica de la clase objetivo de entrenamiento y prueba 
# Evidenciando el desequilibrio de clases
fig, ax = plt.subplots(1, 1, figsize=(10, 7))
ax.bar(*np.unique(y_train, return_counts=True), color="#33F6FF", label="train")
ax.bar(*np.unique(y_test, return_counts=True), color="#33FF55", label="test")
ax.set_xticks([0, 1])
ax.legend()

"""###Implementar los Algoritmos supervisados mas utilizados en la Literatura"""

# Funcion para realizar la matrix de confusion

def list_confusion_matrix(cm,classes):

  cnf_matrix = confusion_matrix(cm[0], cm[1])


  df = pd.DataFrame(data = cnf_matrix,
                    index = pd.MultiIndex.from_product([['Valor real'], classes]),
                    columns = pd.MultiIndex.from_product([['Valor predicho'], classes]))
  cm_display = ConfusionMatrixDisplay(confusion_matrix = confusion_matrix(y_test, Y_pred), display_labels = [False, True])
  cm_display.plot()
  plt.show()
  return df

"""1. Arboles de decisión
2. Random Forest
3. Regresión logística
4. Redes neuronales 
5. kvecinos mas cercanos
6. Naïve Bayes
7. Maquinas de soporte vectorial

#### Arbol de Desicion de clasificacion
"""

# Definir evaluacion Aprox 250 entrenamientos con los siguientes Hiperparametros

HipPDesicionTree= {
    "max_depth": [3,4,5,6,7,8,9,10],
    "criterion": ["gini", "entropy", "poisson"]
}

gs = GridSearchCV(DecisionTreeClassifier(), HipPDesicionTree, scoring=make_scorer(f1_score,greater_is_better=True), cv=LeaveOneOut())

gs.fit(X_train, y_train)

Prueba=pd.DataFrame(gs.cv_results_)
Prueba.columns.unique()

gs.best_estimator_
# Prueba[["rank_test_score","params"]].loc[Prueba.rank_test_score==1]

Y_pred=gs.best_estimator_.predict(X_test)

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))

print(classification_report(y_test,Y_pred))

fig, ax = plt.subplots(figsize=(100, 20))
plot = plot_tree(
            decision_tree = gs.best_estimator_,
            feature_names = dfFiltro.columns,
            class_names   = 'Abandono',
            filled        = True,
            impurity      = False,
            fontsize      = 6,
            ax            = ax
       )

import joblib
joblib.dump(gs.best_estimator_, 'modelo_entrenado.pkl')

"""#### Random Forest"""

HipPRandomForest= {
    "criterion": ("gini", "entropy"),
    "n_estimators": (40,50,100),
    "max_samples":(1/3, 2/3),
    "max_depth": (7,8,9,10)
}

gs = GridSearchCV(RandomForestClassifier(), HipPRandomForest, scoring="recall", cv=LeaveOneOut())

gs.fit(X_train, y_train)

RFDF=pd.DataFrame(gs.cv_results_)
pd.DataFrame(RFDF.loc[RFDF.rank_test_score==1].params)

pd.DataFrame(gs.best_estimator_)

Y_pred=gs.best_estimator_.predict(X_test)

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))
print(classification_report(y_test,Y_pred))

"""#### Regresión logística"""

dfRegresionLogist=pd.get_dummies(dfFiltro,columns=['ESTRATO','ESTADO_CIVIL',
                                                   'EDAD_DE_INGRESO'])
X_dummies=np.array(dfRegresionLogist.drop(['SITE_DESCRIPCION'],1))
y_dummies=np.array(dfRegresionLogist.SITE_DESCRIPCION)
X_train, X_test, y_train, y_test = train_test_split(X_dummies, y_dummies, test_size=0.3, stratify=y_dummies)

# from sklearn import preprocessing

# scaler = preprocessing.MinMaxScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.fit_transform(X_test)

import warnings
warnings.filterwarnings('ignore')

HipPLogisticRegression = {
    'penalty' : ['l1','l2'], 
    'C'       : np.logspace(-3,3,7),
    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],
}


gs = GridSearchCV(LogisticRegression(), HipPLogisticRegression, scoring=make_scorer(f1_score), cv=LeaveOneOut())
gs.fit(X_train, y_train)
Y_pred=gs.best_estimator_.predict(X_test)

gs.best_estimator_

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))

print(classification_report(y_test,Y_pred))

"""#### Redes neuronales"""

# Condiciones del procesamiento 
# Binarización (One hot ecoding) de las variables categóricas
# Convertir a Dummis el df 
# Se escala las variables numericas 

#la primer capa = a la cantidad de caracteristicas del dataframe
print(X_train.shape[1])

# La ultima capa de la red neuronal es la cantidad de clases en la columna objetivo = 2
print(dfFiltro.SITE_DESCRIPCION.nunique())

# Learning rate debe iniciar alto y bajarlo en los hiperparametros

HipPNeuralNetwork = {
    'hidden_layer_sizes':[(200,200),(100,100)],
    'activation' : [ 'logistic', 'relu']
    }
gs = GridSearchCV(MLPClassifier(), HipPNeuralNetwork,
                  scoring=make_scorer(f1_score),cv=LeaveOneOut())

gs.fit(X_train, y_train)
Y_pred=gs.best_estimator_.predict(X_test)

gs.best_estimator_

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))

print(classification_report(y_test,Y_pred))

"""#### Maquinas de soporte vectorial"""

#Maquinas de Soporte vectorial
svc =SVC(C=0.001)
svc.fit(X_train,y_train)
Y_pred=svc.predict(X_test)
print('Precision de Maquinas de sopoprte vectorial')
# print(svc.score(X_train,y_train))
print(classification_report(y_test,Y_pred))

HipPSVC = {
    'C':[0.001,0.005,0.01,],
    'kernel' : [ 'linear', 'poly', 'rbf', 'sigmoid']
    }
gs = GridSearchCV(SVC(), HipPSVC,
                  scoring=make_scorer(f1_score), cv=LeaveOneOut())
gs.fit(X_train, y_train)
Y_pred=gs.best_estimator_.predict(X_test)

gs.best_estimator_

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))

print(classification_report(y_test,Y_pred))

"""## Creación de dataset de entrenamiento y de prueba con SMOTE"""

from unicodedata import normalize
sm= SMOTE(random_state=42)
X_sm, y_sm=sm.fit_resample(X,y)
print(f'Cambio de x antes de SMOTE: {X.shape} \nCambio de X despues de SMOTE {X_sm.shape} ')
print(f'Cambio de y antes de SMOTE: {y.shape} \nCambio de y despues de SMOTE {y_sm.shape} ')

print(f'Balance de de las clases ahora: \n')

pd.Series(y_sm).value_counts(normalize=True)*100

print(X.shape)
print(y.shape)
print(X_sm.shape)
print(y_sm.shape)

X_train, X_test, y_train, y_test = train_test_split(X_sm, y_sm, test_size=0.3, stratify=y_sm)
fig, ax = plt.subplots(1, 1, figsize=(10, 7))
ax.bar(*np.unique(y_train, return_counts=True), color="#33F6FF", label="train")
ax.bar(*np.unique(y_test, return_counts=True), color="#33FF55", label="test")
ax.set_xticks([0, 1])
ax.legend()

Prueba=pd.DataFrame(gs.cv_results_)
Prueba.columns.unique()

"""#### Arbol de Desicion """

# Definir evaluacion Aprox 250 entrenamientos con los siguientes Hiperparametros

HipPDesicionTree= {
    "max_depth": [3,4,5,6,7,8,9,10],
    "criterion": ["gini", "entropy", "poisson"]
}

gs = GridSearchCV(DecisionTreeClassifier(), HipPDesicionTree, scoring=make_scorer(f1_score,greater_is_better=True), cv=LeaveOneOut())

gs.fit(X_train, y_train)

gs.best_estimator_

Prueba=pd.DataFrame(gs.cv_results_)
Prueba.columns.unique()

Y_pred=gs.best_estimator_.predict(X_test)

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))

print(classification_report(y_test,Y_pred))

fig, ax = plt.subplots(figsize=(13, 6))
plot = plot_tree(
            decision_tree = gs.best_estimator_,
            feature_names = dfFiltro.columns,
            class_names   = 'Abandono',
            filled        = True,
            impurity      = False,
            fontsize      = 10,
            ax            = ax
       )

"""#### Random Forest"""

HipPRandomForest= {
    "criterion": ("gini", "entropy"),
    "n_estimators": (40,50,100),
    "max_samples":(1/3, 2/3),
    "max_depth": (7,8,9,10)
}

gs = GridSearchCV(RandomForestClassifier(), HipPRandomForest, scoring="recall", cv=LeaveOneOut())

gs.fit(X_train, y_train)

RFDF=pd.DataFrame(gs.cv_results_)
pd.DataFrame(RFDF.loc[RFDF.rank_test_score==1].params)

pd.DataFrame(RFDF.loc[RFDF.rank_test_score==1].params)

pd.DataFrame(gs.best_estimator_)

gs.best_estimator_

Y_pred=gs.best_estimator_.predict(X_test)

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))
print(classification_report(y_test,Y_pred))

TablaDeImportancia=pd.DataFrame()
RFC=gs.best_estimator_
TablaDeImportancia=pd.Series(RFC.feature_importances_).nlargest(n=40,keep="first").reset_index()
TablaDeImportancia.columns=['index', "Porcentaje"]
TablaDeImportancia["Acomulada"]=TablaDeImportancia.Porcentaje.cumsum()
TablaDeImportancia["Caracteristicas"]=pd.Series(dfFiltro.drop(columns=["SITE_DESCRIPCION"]).columns[[pd.Series(RFC.feature_importances_).nlargest(n=40,keep="first").index]])
# pd.Series(dfDummiCompleto.columns[[pd.Series(RFC.feature_importances_).nlargest(n=10,keep="first").index]])
# pd.Series(RFC.feature_importances_).nlargest(n=10,keep="first").reset_index()
TablaDeImportancia
# TablaDeImportancia

"""#### Regresión logística"""

# from sklearn import preprocessing

# scaler = preprocessing.MinMaxScaler()
# X_train = scaler.fit_transform(X_train)
# X_test = scaler.fit_transform(X_test)

import warnings
warnings.filterwarnings('ignore')

HipPLogisticRegression = {
    'penalty' : ['l1','l2'], 
    'C'       : np.logspace(-3,3,7),
    'solver'  : ['newton-cg', 'lbfgs', 'liblinear'],
}


gs = GridSearchCV(LogisticRegression(), HipPLogisticRegression, scoring=make_scorer(f1_score), cv=LeaveOneOut())
gs.fit(X_train, y_train)
Y_pred=gs.best_estimator_.predict(X_test)

gs.best_estimator_

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))

print(classification_report(y_test,Y_pred))

"""#### Redes neuronales"""

HipPNeuralNetwork = {
    'hidden_layer_sizes':[(200,200),(100,100)],
    'activation' : [ 'logistic', 'relu']
    }
gs = GridSearchCV(MLPClassifier(), HipPNeuralNetwork,
                  scoring=make_scorer(f1_score),cv=LeaveOneOut())

gs.fit(X_train, y_train)
Y_pred=gs.best_estimator_.predict(X_test)

gs.best_estimator_

Y_pred=gs.best_estimator_.predict(X_test)

print(list_confusion_matrix([y_test, Y_pred], dfFiltro.SITE_DESCRIPCION.unique()))
print(classification_report(y_test,Y_pred))

import joblib
joblib.dump(gs.best_estimator_, 'MLPClassifier.pkl')